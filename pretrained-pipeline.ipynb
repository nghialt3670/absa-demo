{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6KVBKBcFio2",
        "outputId": "5b8ed2fe-c066-4c8a-ff92-da26d47068bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \"setfit[absa]\"\n",
        "!pip install -q contractions\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QfwUyHaFFllX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nghialt\\.conda\\envs\\ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\nghialt\\.conda\\envs\\ai\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from setfit import AbsaModel\n",
        "import contractions\n",
        "import nltk\n",
        "import spacy\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwPAPBoeLTJW",
        "outputId": "18e23240-ea64-4ed3-e28c-12eb6bf4969a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nghialt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nghialt\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Tải về các gói cần thiết từ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Tải mô hình ngôn ngữ tiếng Anh\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dr4mtFPnHu_h"
      },
      "outputs": [],
      "source": [
        "def processing_text(text, lower_case=True):\n",
        "    # Preprocess text: lowercasing, contraction expansion, and tokenization\n",
        "    lower_text = text.lower() if lower_case else text\n",
        "    expanded_text = contractions.fix(lower_text)\n",
        "    processed_text = re.sub(r'[^a-zA-Z0-9\\s\\']', ' ', expanded_text)\n",
        "    tokens = nltk.word_tokenize(processed_text)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q_gp3UofWZEz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nghialt\\.conda\\envs\\ai\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nghialt\\.cache\\huggingface\\hub\\models--tomaarsen--setfit-absa-paraphrase-mpnet-base-v2-restaurants-aspect. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\nghialt\\.conda\\envs\\ai\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\nghialt\\.conda\\envs\\ai\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nghialt\\.cache\\huggingface\\hub\\models--tomaarsen--setfit-absa-paraphrase-mpnet-base-v2-restaurants-polarity. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Create a model with a chosen sentence transformer from the Hub\n",
        "model = AbsaModel.from_pretrained(\n",
        "    \"tomaarsen/setfit-absa-paraphrase-mpnet-base-v2-restaurants-aspect\",\n",
        "    \"tomaarsen/setfit-absa-paraphrase-mpnet-base-v2-restaurants-polarity\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pn_2qBwcEQ2P"
      },
      "outputs": [],
      "source": [
        "def detect_span(sentence, query_span):\n",
        "    \"\"\"\n",
        "    Detect the position (start, end) of a query span in a given sentence.\n",
        "    \"\"\"\n",
        "    # Process the sentence using spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Split the query span into tokens\n",
        "    query_tokens = query_span.split()\n",
        "\n",
        "    span_position = None\n",
        "    # Find the position of the span\n",
        "    for token in doc:\n",
        "        # Check if the token matches the first token of the query span\n",
        "        if token.text == query_tokens[0]:\n",
        "            # Attempt to match the entire query span\n",
        "            span = doc[token.i:token.i + len(query_tokens)]\n",
        "            if span.text == query_span:\n",
        "                span_position = (token.i, token.i + len(query_tokens) - 1)  # Inclusive indexing\n",
        "                break\n",
        "    return span_position\n",
        "\n",
        "def predict_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Predict aspects and polarities for a given sentence and label tokens.\n",
        "    \"\"\"\n",
        "    # Preprocess the sentence\n",
        "    processed_sentence = processing_text(sentence)\n",
        "    # processed_sentence = sentence\n",
        "    # Get predictions from the model\n",
        "    pred = model.predict([processed_sentence])[0]\n",
        "\n",
        "    positions, polarities = [], []\n",
        "    for aspect in pred:\n",
        "        # Detect span positions for each aspect term\n",
        "        span_position = detect_span(processed_sentence, aspect['span'])\n",
        "        if span_position:\n",
        "            positions.append(span_position)\n",
        "            # Map polarity to a label\n",
        "            if aspect['polarity'] == 'positive':\n",
        "                polarities.append('POS')\n",
        "            elif aspect['polarity'] == 'negative':\n",
        "                polarities.append('NEG')\n",
        "            elif aspect['polarity'] == 'neutral':\n",
        "                polarities.append('NEU')\n",
        "\n",
        "    prev_end = 0\n",
        "\n",
        "    # Tokenize the processed sentence\n",
        "    tokens = nltk.word_tokenize(processed_sentence)\n",
        "    labels = []\n",
        "\n",
        "    # Assign labels to tokens based on aspect spans and polarities\n",
        "    for (start, end), polarity in zip(positions, polarities):\n",
        "        labels.extend(['O'] * (start - prev_end))\n",
        "        if start == end:\n",
        "            labels.append(f'S-{polarity}')\n",
        "        else:\n",
        "            labels.extend([f'B-{polarity}'] + ['I-' + polarity] * (end - start - 1) + [f'E-{polarity}'])\n",
        "        prev_end = end + 1\n",
        "\n",
        "    # Add 'O' labels for tokens outside any aspect span\n",
        "    labels.extend(['O'] * (len(tokens) - prev_end))\n",
        "\n",
        "    return list(zip(tokens, labels))\n",
        "\n",
        "def predict_paragraph(paragraph):\n",
        "    \"\"\"\n",
        "    Predict aspects and polarities for an entire paragraph.\n",
        "    \"\"\"\n",
        "    # Split the paragraph into sentences using spaCy\n",
        "    doc = nlp(paragraph)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        results.append(predict_sentence(sentence))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaeV6ojeLEaF",
        "outputId": "d0a4eb46-aabf-4adb-e499-37bfd8cb3144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('as', 'O'), ('i', 'O'), ('waited', 'O'), ('in', 'O'), ('line', 'S-NEU')],\n",
              " [('a', 'O'),\n",
              "  ('worker', 'S-NEU'),\n",
              "  ('in', 'O'),\n",
              "  ('a', 'O'),\n",
              "  ('black', 'O'),\n",
              "  ('polo', 'B-NEU'),\n",
              "  ('shirt', 'I-NEU'),\n",
              "  ('supervisor', 'E-NEU'),\n",
              "  ('i', 'O'),\n",
              "  ('think', 'O'),\n",
              "  ('picked', 'O'),\n",
              "  ('his', 'O'),\n",
              "  ('nose', 'O'),\n",
              "  ('pushed', 'O'),\n",
              "  ('down', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('garbage', 'S-NEU'),\n",
              "  ('picked', 'O'),\n",
              "  ('up', 'O'),\n",
              "  ('a', 'O'),\n",
              "  ('piece', 'O'),\n",
              "  ('of', 'O'),\n",
              "  ('trash', 'S-NEU'),\n",
              "  ('and', 'O'),\n",
              "  ('fixed', 'O'),\n",
              "  ('his', 'O'),\n",
              "  ('pants', 'O'),\n",
              "  ('with', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('same', 'O'),\n",
              "  ('hand', 'O'),\n",
              "  ('he', 'O'),\n",
              "  ('had', 'O'),\n",
              "  ('a', 'O'),\n",
              "  ('glove', 'O'),\n",
              "  ('on', 'O'),\n",
              "  ('then', 'O'),\n",
              "  ('went', 'O'),\n",
              "  ('back', 'O'),\n",
              "  ('to', 'O'),\n",
              "  ('serving', 'O'),\n",
              "  ('food', 'S-NEU')],\n",
              " [('i', 'O'),\n",
              "  ('asked', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('girl', 'O'),\n",
              "  ('helping', 'O'),\n",
              "  ('me', 'O'),\n",
              "  ('if', 'O'),\n",
              "  ('he', 'O'),\n",
              "  ('could', 'O'),\n",
              "  ('change', 'O'),\n",
              "  ('his', 'O'),\n",
              "  ('gloves', 'S-NEU'),\n",
              "  ('please', 'O'),\n",
              "  ('as', 'O'),\n",
              "  ('he', 'O'),\n",
              "  ('was', 'O'),\n",
              "  ('about', 'O'),\n",
              "  ('to', 'O'),\n",
              "  ('touch', 'O'),\n",
              "  ('my', 'O'),\n",
              "  ('plate', 'S-NEU'),\n",
              "  ('she', 'O'),\n",
              "  ('told', 'O'),\n",
              "  ('him', 'O'),\n",
              "  ('and', 'O'),\n",
              "  ('he', 'O'),\n",
              "  ('just', 'O'),\n",
              "  ('walked', 'O'),\n",
              "  ('around', 'O'),\n",
              "  ('until', 'O'),\n",
              "  ('i', 'O'),\n",
              "  ('left', 'O')],\n",
              " [('with', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('same', 'O'),\n",
              "  ('glove', 'S-POS'),\n",
              "  ('on', 'O'),\n",
              "  ('she', 'O'),\n",
              "  ('finished', 'O'),\n",
              "  ('putting', 'O'),\n",
              "  ('together', 'O'),\n",
              "  ('my', 'O'),\n",
              "  ('plate', 'S-NEU')],\n",
              " [('he', 'O'),\n",
              "  ('never', 'O'),\n",
              "  ('changed', 'O'),\n",
              "  ('his', 'O'),\n",
              "  ('glove', 'S-POS'),\n",
              "  ('as', 'O'),\n",
              "  ('far', 'O'),\n",
              "  ('as', 'O'),\n",
              "  ('i', 'O'),\n",
              "  ('know', 'O')],\n",
              " [('terrible', 'O'), ('food', 'B-NEG'), ('safety', 'E-NEG')]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"As I waited in line. A worker in a black polo shirt (supervisor I think?) picked his nose, pushed down the garbage, picked up a piece of trash, and fixed his pants with the same hand he had a glove on, then went back to serving food. I asked the girl helping me if he could change his gloves please as he was about to touch my plate, she told him, and he just walked around until I left.. with the same glove on… She finished putting together my plate. He never changed his glove as far as I know. Terrible food safety.\"\n",
        "predict_paragraph(sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
